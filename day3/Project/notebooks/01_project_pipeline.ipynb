{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ Day 3 ì¡°ë³„ í”„ë¡œì íŠ¸: E2E MLOps Pipeline\n",
    "\n",
    "## ğŸ“‹ ê°œìš”\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” 3ì¼ê°„ í•™ìŠµí•œ MLOps ê¸°ìˆ ì„ ì¢…í•©í•˜ì—¬ ì™„ì „í•œ E2E ML íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "\n",
    "### íŒŒì´í”„ë¼ì¸ êµ¬ì¡°\n",
    "```\n",
    "Load Data â†’ Preprocess â†’ Feature Engineering â†’ Train Model â†’ Evaluate â†’ Deploy/Alert\n",
    "```\n",
    "\n",
    "### âš ï¸ ì¤‘ìš”: AWS ìê²©ì¦ëª… ì„¤ì • í•„ìš”\n",
    "MLflow ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ë¥¼ S3ì— ì €ì¥í•˜ë ¤ë©´ AWS ìê²©ì¦ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install kfp==2.7.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kfp import dsl\n",
    "from kfp.dsl import component, Input, Output, Dataset\n",
    "from kfp import compiler\n",
    "\n",
    "print(\"[OK] KFP SDK loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TODO: Change to your team settings!\n",
    "# ============================================================\n",
    "\n",
    "TEAM_NAME = \"team-01\"  # e.g., team-01, team-02, team-03, team-04\n",
    "\n",
    "def get_current_namespace():\n",
    "    \"\"\"Get the namespace where the current Pod is running.\"\"\"\n",
    "    namespace_path = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n",
    "    if os.path.exists(namespace_path):\n",
    "        with open(namespace_path, \"r\") as f:\n",
    "            return f.read().strip()\n",
    "    if os.environ.get(\"KF_PIPELINES_NAMESPACE\"):\n",
    "        return os.environ.get(\"KF_PIPELINES_NAMESPACE\")\n",
    "    return \"kubeflow-user20\"\n",
    "\n",
    "USER_NAMESPACE = get_current_namespace()\n",
    "MLFLOW_TRACKING_URI = f\"http://mlflow-server.{USER_NAMESPACE}.svc.cluster.local:5000\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  Project Settings\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Team Name: {TEAM_NAME}\")\n",
    "print(f\"  Namespace: {USER_NAMESPACE}\")\n",
    "print(f\"  MLflow URI: {MLFLOW_TRACKING_URI}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. ì»´í¬ë„ŒíŠ¸ ì •ì˜\n",
    "\n",
    "### 2.1 Component 1: ë°ì´í„° ë¡œë“œ\n",
    "\n",
    "California Housing ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"pandas==2.0.3\", \"scikit-learn==1.3.2\"]\n",
    ")\n",
    "def load_data(\n",
    "    data_source: str,\n",
    "    output_data: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Load California Housing dataset\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.datasets import fetch_california_housing\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Step 1: Load Data\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    housing = fetch_california_housing(as_frame=True)\n",
    "    df = housing.frame\n",
    "    \n",
    "    print(f\"  Source: {data_source}\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    \n",
    "    df.to_csv(output_data.path, index=False)\n",
    "    print(\"  [OK] Data saved\")\n",
    "\n",
    "print(\"[OK] load_data component defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Component 2: ì „ì²˜ë¦¬\n",
    "\n",
    "ë°ì´í„°ë¥¼ Train/Testë¡œ ë¶„í• í•˜ê³  StandardScalerë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"pandas==2.0.3\", \"scikit-learn==1.3.2\", \"numpy==1.24.3\"]\n",
    ")\n",
    "def preprocess(\n",
    "    input_data: Input[Dataset],\n",
    "    X_train_out: Output[Dataset],\n",
    "    X_test_out: Output[Dataset],\n",
    "    y_train_out: Output[Dataset],\n",
    "    y_test_out: Output[Dataset],\n",
    "    test_size: float = 0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    Data preprocessing: Train/Test split and normalization\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Step 2: Preprocess\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df = pd.read_csv(input_data.path)\n",
    "    \n",
    "    X = df.drop(columns=['MedHouseVal'])\n",
    "    y = df['MedHouseVal']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_test_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "    \n",
    "    print(f\"  Train: {X_train_df.shape}\")\n",
    "    print(f\"  Test: {X_test_df.shape}\")\n",
    "    \n",
    "    X_train_df.to_csv(X_train_out.path, index=False)\n",
    "    X_test_df.to_csv(X_test_out.path, index=False)\n",
    "    pd.DataFrame(y_train).to_csv(y_train_out.path, index=False)\n",
    "    pd.DataFrame(y_test).to_csv(y_test_out.path, index=False)\n",
    "    \n",
    "    print(\"  [OK] Preprocessing completed\")\n",
    "\n",
    "print(\"[OK] preprocess component defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Component 3: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
    "\n",
    "âš ï¸ **TODO**: ì´ ë¶€ë¶„ì—ì„œ **ìµœì†Œ 1ê°œ ì´ìƒì˜ íŒŒìƒ í”¼ì²˜ë¥¼ ì¶”ê°€**í•´ì•¼ í•©ë‹ˆë‹¤!\n",
    "\n",
    "#### í”¼ì²˜ ì•„ì´ë””ì–´ ì˜ˆì‹œ\n",
    "\n",
    "| í”¼ì²˜ëª… | ê³„ì‚°ì‹ | ì„¤ëª… |\n",
    "|--------|--------|------|\n",
    "| `bedroom_ratio` | AveBedrms / AveRooms | ì¹¨ì‹¤ ë¹„ìœ¨ |\n",
    "| `rooms_per_person` | AveRooms / AveOccup | ì¸ë‹¹ ë°© ìˆ˜ |\n",
    "| `population_density` | Population * AveOccup | ì¸êµ¬ ë°€ë„ |\n",
    "| `income_age_ratio` | MedInc / HouseAge | ì†Œë“ ëŒ€ë¹„ ì£¼íƒ ì—°ë ¹ |\n",
    "| `dist_to_bay` | sqrt((Lat-37.87)^2 + (Lon+122.27)^2) | Bay Area ê±°ë¦¬ |\n",
    "| `dist_to_la` | sqrt((Lat-34.05)^2 + (Lon+118.24)^2) | LA ê±°ë¦¬ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"pandas==2.0.3\", \"numpy==1.24.3\"]\n",
    ")\n",
    "def feature_engineering(\n",
    "    X_train_in: Input[Dataset],\n",
    "    X_test_in: Input[Dataset],\n",
    "    X_train_out: Output[Dataset],\n",
    "    X_test_out: Output[Dataset]\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Feature Engineering: Create derived features\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of features created\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Step 3: Feature Engineering\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    X_train = pd.read_csv(X_train_in.path)\n",
    "    X_test = pd.read_csv(X_test_in.path)\n",
    "    \n",
    "    original_cols = list(X_train.columns)\n",
    "    print(f\"  Original features: {len(original_cols)}\")\n",
    "    \n",
    "    def add_features(df):\n",
    "        # ============================================================\n",
    "        # TODO: Add more features for your team! (at least 1)\n",
    "        # ============================================================\n",
    "        \n",
    "        # Example 1: Bedroom ratio\n",
    "        df['bedroom_ratio'] = df['AveBedrms'] / (df['AveRooms'] + 1e-6)\n",
    "        \n",
    "        # Example 2: TODO - Add your feature!\n",
    "        # df['rooms_per_person'] = df['AveRooms'] / (df['AveOccup'] + 1e-6)\n",
    "        \n",
    "        # Example 3: TODO - Add your feature!\n",
    "        # df['density'] = df['Population'] * df['AveOccup']\n",
    "        \n",
    "        # ============================================================\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    X_train_fe = add_features(X_train)\n",
    "    X_test_fe = add_features(X_test)\n",
    "    \n",
    "    new_cols = [c for c in X_train_fe.columns if c not in original_cols]\n",
    "    print(f\"  New features ({len(new_cols)}):\")\n",
    "    for feat in new_cols:\n",
    "        stats = X_train_fe[feat].describe()\n",
    "        print(f\"    - {feat}: mean={stats['mean']:.4f}, std={stats['std']:.4f}\")\n",
    "    \n",
    "    print(f\"  Total features: {len(X_train_fe.columns)}\")\n",
    "    \n",
    "    X_train_fe.to_csv(X_train_out.path, index=False)\n",
    "    X_test_fe.to_csv(X_test_out.path, index=False)\n",
    "    \n",
    "    print(\"  [OK] Feature engineering completed\")\n",
    "    \n",
    "    return len(new_cols)\n",
    "\n",
    "print(\"[OK] feature_engineering component defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Component 4: ëª¨ë¸ í•™ìŠµ\n",
    "\n",
    "RandomForest ëª¨ë¸ì„ í•™ìŠµí•˜ê³  MLflowì— ê¸°ë¡í•©ë‹ˆë‹¤.\n",
    "\n",
    "âœ… **AWS ìê²©ì¦ëª… íŒŒë¼ë¯¸í„° í¬í•¨**: S3ì— ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.0.3\",\n",
    "        \"scikit-learn==1.3.2\",\n",
    "        \"mlflow==2.9.2\",\n",
    "        \"numpy==1.24.3\",\n",
    "        \"boto3\"\n",
    "    ]\n",
    ")\n",
    "def train_model(\n",
    "    X_train: Input[Dataset],\n",
    "    X_test: Input[Dataset],\n",
    "    y_train: Input[Dataset],\n",
    "    y_test: Input[Dataset],\n",
    "    mlflow_tracking_uri: str,\n",
    "    experiment_name: str,\n",
    "    team_name: str,\n",
    "    aws_access_key_id: str,\n",
    "    aws_secret_access_key: str,\n",
    "    aws_region: str = \"ap-northeast-2\",\n",
    "    n_estimators: int = 100,\n",
    "    max_depth: int = 10\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Train model and log to MLflow\n",
    "    \n",
    "    Args:\n",
    "        aws_access_key_id: AWS Access Key ID (for S3)\n",
    "        aws_secret_access_key: AWS Secret Access Key (for S3)\n",
    "        aws_region: AWS Region\n",
    "    \n",
    "    Returns:\n",
    "        str: MLflow Run ID\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import mlflow\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "    import os\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Step 4: Train Model - {team_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Set AWS credentials for S3 upload\n",
    "    os.environ['AWS_ACCESS_KEY_ID'] = aws_access_key_id\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = aws_secret_access_key\n",
    "    os.environ['AWS_DEFAULT_REGION'] = aws_region\n",
    "    print(f\"  AWS Region: {aws_region}\")\n",
    "    print(f\"  AWS Credentials: {'[OK]' if aws_access_key_id else '[ERROR] Not set'}\")\n",
    "    \n",
    "    # Load data\n",
    "    X_train_df = pd.read_csv(X_train.path)\n",
    "    X_test_df = pd.read_csv(X_test.path)\n",
    "    y_train_df = pd.read_csv(y_train.path)\n",
    "    y_test_df = pd.read_csv(y_test.path)\n",
    "    \n",
    "    print(f\"  Training data: {X_train_df.shape}\")\n",
    "    print(f\"  Test data: {X_test_df.shape}\")\n",
    "    \n",
    "    # MLflow settings\n",
    "    os.environ['MLFLOW_TRACKING_URI'] = mlflow_tracking_uri\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    print(f\"  MLflow URI: {mlflow_tracking_uri}\")\n",
    "    print(f\"  Experiment: {experiment_name}\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"{team_name}-run\") as run:\n",
    "        run_id = run.info.run_id\n",
    "        print(f\"  MLflow Run ID: {run_id}\")\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params({\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"max_depth\": max_depth,\n",
    "            \"random_state\": 42,\n",
    "            \"n_features\": X_train_df.shape[1],\n",
    "            \"n_samples_train\": X_train_df.shape[0],\n",
    "            \"n_samples_test\": X_test_df.shape[0]\n",
    "        })\n",
    "        mlflow.set_tag(\"team\", team_name)\n",
    "        \n",
    "        # Train model\n",
    "        print(f\"  Training RandomForest (n_estimators={n_estimators}, max_depth={max_depth})...\")\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train_df, y_train_df.values.ravel())\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = model.predict(X_test_df)\n",
    "        \n",
    "        r2 = r2_score(y_test_df, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_df, y_pred))\n",
    "        mae = mean_absolute_error(y_test_df, y_pred)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metrics({\"r2\": r2, \"rmse\": rmse, \"mae\": mae})\n",
    "        \n",
    "        print(f\"  Performance:\")\n",
    "        print(f\"    - R2 Score: {r2:.4f}\")\n",
    "        print(f\"    - RMSE: {rmse:.4f}\")\n",
    "        print(f\"    - MAE: {mae:.4f}\")\n",
    "        \n",
    "        # Feature importance\n",
    "        feature_importance = dict(zip(\n",
    "            X_train_df.columns,\n",
    "            model.feature_importances_\n",
    "        ))\n",
    "        sorted_importance = sorted(\n",
    "            feature_importance.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )[:5]\n",
    "        \n",
    "        print(f\"  Top 5 Feature Importance:\")\n",
    "        for feat, imp in sorted_importance:\n",
    "            safe_name = feat.replace(\" \", \"_\")[:15]\n",
    "            mlflow.log_metric(f\"fi_{safe_name}\", imp)\n",
    "            print(f\"    - {feat}: {imp:.4f}\")\n",
    "        \n",
    "        # Save model to S3 via MLflow\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        print(\"  [OK] Model saved to MLflow (S3)\")\n",
    "    \n",
    "    return run_id\n",
    "\n",
    "print(\"[OK] train_model component defined (with AWS credentials)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Component 5: ëª¨ë¸ í‰ê°€\n",
    "\n",
    "R2 Scoreê°€ ì„ê³„ê°’(ê¸°ë³¸ 0.75) ì´ìƒì´ë©´ ë°°í¬, ë¯¸ë§Œì´ë©´ ì•Œë¦¼ì„ ë°œì†¡í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"mlflow==2.9.2\", \"boto3\"]\n",
    ")\n",
    "def evaluate_model(\n",
    "    run_id: str,\n",
    "    mlflow_tracking_uri: str,\n",
    "    r2_threshold: float = 0.75\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Evaluate model and decide deployment\n",
    "    \n",
    "    Returns:\n",
    "        str: \"deploy\" or \"skip\"\n",
    "    \"\"\"\n",
    "    import mlflow\n",
    "    import os\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Step 5: Evaluate Model\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    os.environ['MLFLOW_TRACKING_URI'] = mlflow_tracking_uri\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "    \n",
    "    run = mlflow.get_run(run_id)\n",
    "    r2 = run.data.metrics.get(\"r2\", 0)\n",
    "    rmse = run.data.metrics.get(\"rmse\", 0)\n",
    "    \n",
    "    print(f\"  Run ID: {run_id}\")\n",
    "    print(f\"  R2 Score: {r2:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  Threshold: {r2_threshold}\")\n",
    "    \n",
    "    if r2 >= r2_threshold:\n",
    "        print(f\"  [OK] R2 >= {r2_threshold} -> DEPLOY\")\n",
    "        return \"deploy\"\n",
    "    else:\n",
    "        print(f\"  [WARN] R2 < {r2_threshold} -> SKIP\")\n",
    "        return \"skip\"\n",
    "\n",
    "print(\"[OK] evaluate_model component defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Component 6: ëª¨ë¸ ë°°í¬ (KServe)\n",
    "\n",
    "ì„±ëŠ¥ì´ ì„ê³„ê°’ ì´ìƒì¼ ë•Œ KServeë¥¼ í†µí•´ ëª¨ë¸ì„ ë°°í¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"kubernetes\"]\n",
    ")\n",
    "def deploy_model(\n",
    "    run_id: str,\n",
    "    model_name: str,\n",
    "    namespace: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Print deployment info for KServe\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Step 6: Deploy Model (KServe)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Run ID: {run_id}\")\n",
    "    print(f\"  Model Name: {model_name}\")\n",
    "    print(f\"  Namespace: {namespace}\")\n",
    "    print(\"\")\n",
    "    print(\"  Deployment Info:\")\n",
    "    print(f\"  Endpoint: http://{model_name}.{namespace}.svc.cluster.local/v1/models/{model_name}:predict\")\n",
    "    print(\"  [OK] Deployment info logged\")\n",
    "\n",
    "print(\"[OK] deploy_model component defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Component 7: ì•Œë¦¼ (ì„±ëŠ¥ ë¯¸ë‹¬ ì‹œ)\n",
    "\n",
    "ì„±ëŠ¥ì´ ì„ê³„ê°’ ë¯¸ë§Œì¼ ë•Œ ì•Œë¦¼ì„ ë°œì†¡í•˜ê³  ê°œì„  ê¶Œê³ ì‚¬í•­ì„ ì œì‹œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9-slim\")\n",
    "def send_alert(\n",
    "    run_id: str,\n",
    "    team_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Alert for performance below threshold\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Alert - {team_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"  [WARN] Model did not meet performance threshold\")\n",
    "    print(f\"  Run ID: {run_id}\")\n",
    "    print(\"  Recommendations:\")\n",
    "    print(\"    1. Add more features\")\n",
    "    print(\"    2. Tune hyperparameters\")\n",
    "    print(\"    3. Try different algorithms\")\n",
    "\n",
    "print(\"[OK] send_alert component defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. íŒŒì´í”„ë¼ì¸ ì •ì˜\n",
    "\n",
    "7ê°œì˜ ì»´í¬ë„ŒíŠ¸ë¥¼ ì—°ê²°í•˜ì—¬ ì™„ì „í•œ E2E íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "âœ… **AWS ìê²©ì¦ëª… íŒŒë¼ë¯¸í„° í¬í•¨**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=f\"{TEAM_NAME}-pipeline\",\n",
    "    description=f\"E2E ML Pipeline by {TEAM_NAME}\"\n",
    ")\n",
    "def project_pipeline(\n",
    "    data_source: str = \"sklearn\",\n",
    "    team_name: str = TEAM_NAME,\n",
    "    experiment_name: str = f\"{TEAM_NAME}-experiment\",\n",
    "    model_name: str = f\"{TEAM_NAME}-model\",\n",
    "    namespace: str = USER_NAMESPACE,\n",
    "    mlflow_tracking_uri: str = MLFLOW_TRACKING_URI,\n",
    "    aws_access_key_id: str = \"\",\n",
    "    aws_secret_access_key: str = \"\",\n",
    "    aws_region: str = \"ap-northeast-2\",\n",
    "    n_estimators: int = 100,\n",
    "    max_depth: int = 10,\n",
    "    r2_threshold: float = 0.75\n",
    "):\n",
    "    \"\"\"\n",
    "    E2E ML Pipeline\n",
    "    \n",
    "    Flow:\n",
    "    1. Load Data\n",
    "    2. Preprocess\n",
    "    3. Feature Engineering\n",
    "    4. Train Model (with MLflow + S3)\n",
    "    5. Evaluate Model\n",
    "    6. Deploy (if R2 >= threshold) or Alert (if R2 < threshold)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Load Data\n",
    "    load_task = load_data(data_source=data_source)\n",
    "    \n",
    "    # Step 2: Preprocess\n",
    "    preprocess_task = preprocess(\n",
    "        input_data=load_task.outputs[\"output_data\"]\n",
    "    )\n",
    "    \n",
    "    # Step 3: Feature Engineering\n",
    "    feature_task = feature_engineering(\n",
    "        X_train_in=preprocess_task.outputs[\"X_train_out\"],\n",
    "        X_test_in=preprocess_task.outputs[\"X_test_out\"]\n",
    "    )\n",
    "    \n",
    "    # Step 4: Train Model (with AWS credentials)\n",
    "    train_task = train_model(\n",
    "        X_train=feature_task.outputs[\"X_train_out\"],\n",
    "        X_test=feature_task.outputs[\"X_test_out\"],\n",
    "        y_train=preprocess_task.outputs[\"y_train_out\"],\n",
    "        y_test=preprocess_task.outputs[\"y_test_out\"],\n",
    "        mlflow_tracking_uri=mlflow_tracking_uri,\n",
    "        experiment_name=experiment_name,\n",
    "        team_name=team_name,\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "        aws_region=aws_region,\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth\n",
    "    )\n",
    "    \n",
    "    # Step 5: Evaluate\n",
    "    evaluate_task = evaluate_model(\n",
    "        run_id=train_task.output,\n",
    "        mlflow_tracking_uri=mlflow_tracking_uri,\n",
    "        r2_threshold=r2_threshold\n",
    "    )\n",
    "    \n",
    "    # Step 6: Conditional Deploy or Alert\n",
    "    with dsl.If(evaluate_task.output == \"deploy\"):\n",
    "        deploy_model(\n",
    "            run_id=train_task.output,\n",
    "            model_name=model_name,\n",
    "            namespace=namespace\n",
    "        )\n",
    "    \n",
    "    with dsl.If(evaluate_task.output == \"skip\"):\n",
    "        send_alert(\n",
    "            run_id=train_task.output,\n",
    "            team_name=team_name\n",
    "        )\n",
    "\n",
    "print(\"[OK] project_pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. íŒŒì´í”„ë¼ì¸ ì»´íŒŒì¼\n",
    "\n",
    "íŒŒì´í”„ë¼ì¸ì„ YAML íŒŒì¼ë¡œ ì»´íŒŒì¼í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_file = f\"{TEAM_NAME}_pipeline.yaml\"\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=project_pipeline,\n",
    "    package_path=pipeline_file\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  Pipeline Compiled!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  File: {pipeline_file}\")\n",
    "print(f\"  Team: {TEAM_NAME}\")\n",
    "print(\"\")\n",
    "print(\"  Required parameters when running:\")\n",
    "print(\"    - aws_access_key_id: AWS Access Key\")\n",
    "print(\"    - aws_secret_access_key: AWS Secret Key\")\n",
    "print(\"    - aws_region: ap-northeast-2 (default)\")\n",
    "print(\"\")\n",
    "print(\"  Next Steps:\")\n",
    "print(\"  1. Kubeflow Dashboard -> Pipelines -> Upload pipeline\")\n",
    "print(f\"  2. Upload {pipeline_file}\")\n",
    "print(\"  3. Create Run with AWS credentials\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. (ì„ íƒ) íŒŒì´í”„ë¼ì¸ ì§ì ‘ ì‹¤í–‰\n",
    "\n",
    "Kubeflow SDKë¥¼ í†µí•´ íŒŒì´í”„ë¼ì¸ì„ ì§ì ‘ ì‹¤í–‰í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### âš ï¸ ì¤‘ìš”: ì•„ë˜ ì…€ì—ì„œ AWS ìê²©ì¦ëª…ì„ ì…ë ¥í•˜ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AWS Credentials (Required!)\n",
    "# ============================================================\n",
    "# Enter the AWS credentials provided by the instructor.\n",
    "\n",
    "AWS_ACCESS_KEY_ID = \"\"      # e.g., \"AKIAXXXXXXXXXXXXXXXX\"\n",
    "AWS_SECRET_ACCESS_KEY = \"\"  # e.g., \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "AWS_REGION = \"ap-northeast-2\"\n",
    "\n",
    "if AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY:\n",
    "    print(\"[OK] AWS credentials set!\")\n",
    "else:\n",
    "    print(\"[ERROR] Please enter AWS credentials!\")\n",
    "    print(\"   Set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline directly\n",
    "RUN_PIPELINE = False  # Change to True to run pipeline\n",
    "\n",
    "if RUN_PIPELINE:\n",
    "    if not AWS_ACCESS_KEY_ID or not AWS_SECRET_ACCESS_KEY:\n",
    "        print(\"[ERROR] AWS credentials not set!\")\n",
    "        print(\"   Enter AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY above.\")\n",
    "    else:\n",
    "        from kfp.client import Client\n",
    "        \n",
    "        client = Client()\n",
    "        \n",
    "        run = client.create_run_from_pipeline_func(\n",
    "            project_pipeline,\n",
    "            arguments={\n",
    "                \"team_name\": TEAM_NAME,\n",
    "                \"experiment_name\": f\"{TEAM_NAME}-experiment\",\n",
    "                \"model_name\": f\"{TEAM_NAME}-model\",\n",
    "                \"namespace\": USER_NAMESPACE,\n",
    "                \"mlflow_tracking_uri\": MLFLOW_TRACKING_URI,\n",
    "                \"aws_access_key_id\": AWS_ACCESS_KEY_ID,\n",
    "                \"aws_secret_access_key\": AWS_SECRET_ACCESS_KEY,\n",
    "                \"aws_region\": AWS_REGION,\n",
    "                \"n_estimators\": 100,\n",
    "                \"max_depth\": 10,\n",
    "                \"r2_threshold\": 0.75\n",
    "            },\n",
    "            experiment_name=f\"{TEAM_NAME}-experiments\"\n",
    "        )\n",
    "        \n",
    "        print(\"[OK] Pipeline run started!\")\n",
    "        print(f\"   Run ID: {run.run_id}\")\n",
    "        print(\"   Check progress in Kubeflow Dashboard.\")\n",
    "else:\n",
    "    print(\"[INFO] Direct pipeline execution is disabled.\")\n",
    "    print(\"   Set RUN_PIPELINE = True, or\")\n",
    "    print(\"   Upload the yaml file in Kubeflow Dashboard.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "### í™˜ê²½ ì„¤ì •\n",
    "- [ ] TEAM_NAME ë³€ê²½ (team-01, team-02, ...)\n",
    "- [ ] AWS_ACCESS_KEY_ID ì„¤ì •\n",
    "- [ ] AWS_SECRET_ACCESS_KEY ì„¤ì •\n",
    "\n",
    "### ì»´í¬ë„ŒíŠ¸ êµ¬í˜„\n",
    "- [ ] feature_engineeringì—ì„œ ìµœì†Œ 1ê°œ í”¼ì²˜ ì¶”ê°€\n",
    "\n",
    "### íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "- [ ] íŒŒì´í”„ë¼ì¸ ì»´íŒŒì¼ ì„±ê³µ\n",
    "- [ ] Kubeflow Dashboardì— ì—…ë¡œë“œ\n",
    "- [ ] Run ìƒì„± ì‹œ AWS ìê²©ì¦ëª… ì…ë ¥\n",
    "- [ ] íŒŒì´í”„ë¼ì¸ Succeeded ìƒíƒœ í™•ì¸\n",
    "- [ ] MLflow UIì—ì„œ ì‹¤í—˜ ê²°ê³¼ í™•ì¸\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…\n",
    "\n",
    "### NoCredentialsError\n",
    "```\n",
    "botocore.exceptions.NoCredentialsError: Unable to locate credentials\n",
    "```\n",
    "**í•´ê²°**: íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œ `aws_access_key_id`ì™€ `aws_secret_access_key` íŒŒë¼ë¯¸í„°ì— ê°’ì„ ì…ë ¥í•˜ì„¸ìš”.\n",
    "\n",
    "### RBAC 403 Error\n",
    "```\n",
    "MlflowException: RBAC: access denied\n",
    "```\n",
    "**í•´ê²°**: ê°•ì‚¬ì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”. MLflow ì„œë²„ì˜ Istio sidecar ë¹„í™œì„±í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "### UTF-8 Encoding Error\n",
    "```\n",
    "Conversion from collation utf8mb3_general_ci into utf8mb4_0900_ai_ci impossible\n",
    "```\n",
    "**í•´ê²°**: íŒŒì´í”„ë¼ì¸ ì½”ë“œì—ì„œ í•œê¸€, ì´ëª¨ì§€ë¥¼ ì œê±°í•˜ê³  ì˜ë¬¸ë§Œ ì‚¬ìš©í•˜ì„¸ìš”."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
