{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ Day 3 í”„ë¡œì íŠ¸ ì†”ë£¨ì…˜: E2E MLOps Pipeline\n",
    "\n",
    "## ğŸ“‹ ê°œìš”\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ í”„ë¡œì íŠ¸ì˜ ì™„ì„±ëœ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤. 7ê°œì˜ ì—”ì§€ë‹ˆì–´ë§ëœ í”¼ì²˜ì™€ ì™„ì „í•œ MLflow í†µí•©ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "\n",
    "### âš ï¸ ìˆ˜ì • ì‚¬í•­ (2025-12-17)\n",
    "- AWS S3 ìê²©ì¦ëª… íŒŒë¼ë¯¸í„° ì¶”ê°€ (NoCredentialsError í•´ê²°)\n",
    "- MLflow ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ì¥ì„ ìœ„í•œ AWS í™˜ê²½ë³€ìˆ˜ ì„¤ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kfp==2.7.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kfp import dsl\n",
    "from kfp.dsl import component, Input, Output, Dataset\n",
    "from kfp import compiler\n",
    "\n",
    "print(f\"KFP SDK loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# âš ï¸ ì„¤ì • ë³€ê²½\n",
    "# ============================================================\n",
    "\n",
    "TEAM_NAME = \"solution-team\"\n",
    "\n",
    "def get_current_namespace():\n",
    "    namespace_path = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n",
    "    if os.path.exists(namespace_path):\n",
    "        with open(namespace_path, \"r\") as f:\n",
    "            return f.read().strip()\n",
    "    return \"kubeflow-user20\"\n",
    "\n",
    "USER_NAMESPACE = get_current_namespace()\n",
    "MLFLOW_TRACKING_URI = f\"http://mlflow-server.{USER_NAMESPACE}.svc.cluster.local:5000\"\n",
    "\n",
    "# ============================================================\n",
    "# âš ï¸ AWS ìê²©ì¦ëª… ì„¤ì •\n",
    "# ============================================================\n",
    "AWS_ACCESS_KEY_ID = \"\"      # ë³¸ì¸ì˜ Access Key\n",
    "AWS_SECRET_ACCESS_KEY = \"\"  # ë³¸ì¸ì˜ Secret Key\n",
    "AWS_REGION = \"ap-northeast-2\"\n",
    "\n",
    "print(f\"Team: {TEAM_NAME}\")\n",
    "print(f\"Namespace: {USER_NAMESPACE}\")\n",
    "print(f\"MLflow: {MLFLOW_TRACKING_URI}\")\n",
    "print(f\"AWS Key: {'âœ…' if AWS_ACCESS_KEY_ID else 'âŒ ë¯¸ì„¤ì •'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. ì»´í¬ë„ŒíŠ¸ ì •ì˜\n",
    "\n",
    "### 2.1 Component 1: ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"pandas==2.0.3\", \"scikit-learn==1.3.2\"]\n",
    ")\n",
    "def load_data(\n",
    "    data_source: str,\n",
    "    output_data: Output[Dataset]\n",
    "):\n",
    "    \"\"\"California Housing ë°ì´í„°ì…‹ ë¡œë“œ\"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.datasets import fetch_california_housing\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Step 1: Load Data\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    housing = fetch_california_housing(as_frame=True)\n",
    "    df = housing.frame\n",
    "    \n",
    "    print(f\"  Source: {data_source}\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    \n",
    "    df.to_csv(output_data.path, index=False)\n",
    "    print(f\"  âœ… Data saved\")\n",
    "\n",
    "print(\"âœ… load_data ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Component 2: ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"pandas==2.0.3\", \"scikit-learn==1.3.2\", \"numpy==1.24.3\"]\n",
    ")\n",
    "def preprocess(\n",
    "    input_data: Input[Dataset],\n",
    "    X_train_out: Output[Dataset],\n",
    "    X_test_out: Output[Dataset],\n",
    "    y_train_out: Output[Dataset],\n",
    "    y_test_out: Output[Dataset],\n",
    "    test_size: float = 0.2\n",
    "):\n",
    "    \"\"\"ë°ì´í„° ì „ì²˜ë¦¬\"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Step 2: Preprocess\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df = pd.read_csv(input_data.path)\n",
    "    \n",
    "    X = df.drop(columns=['MedHouseVal'])\n",
    "    y = df['MedHouseVal']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_test_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "    \n",
    "    print(f\"  Train: {X_train_df.shape}, Test: {X_test_df.shape}\")\n",
    "    \n",
    "    X_train_df.to_csv(X_train_out.path, index=False)\n",
    "    X_test_df.to_csv(X_test_out.path, index=False)\n",
    "    pd.DataFrame(y_train).to_csv(y_train_out.path, index=False)\n",
    "    pd.DataFrame(y_test).to_csv(y_test_out.path, index=False)\n",
    "    \n",
    "    print(f\"  âœ… Preprocessing completed\")\n",
    "\n",
    "print(\"âœ… preprocess ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Component 3: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (7ê°œ í”¼ì²˜ - ì™„ì„± ë²„ì „)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"pandas==2.0.3\", \"numpy==1.24.3\"]\n",
    ")\n",
    "def feature_engineering(\n",
    "    X_train_in: Input[Dataset],\n",
    "    X_test_in: Input[Dataset],\n",
    "    X_train_out: Output[Dataset],\n",
    "    X_test_out: Output[Dataset]\n",
    ") -> int:\n",
    "    \"\"\"í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (ì™„ì„± ë²„ì „ - 7ê°œ í”¼ì²˜)\"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Step 3: Feature Engineering (Solution)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    X_train = pd.read_csv(X_train_in.path)\n",
    "    X_test = pd.read_csv(X_test_in.path)\n",
    "    \n",
    "    original_cols = list(X_train.columns)\n",
    "    print(f\"  Original features: {len(original_cols)}\")\n",
    "    \n",
    "    def add_features(df):\n",
    "        # 1. ì¹¨ì‹¤ ë¹„ìœ¨\n",
    "        df['bedroom_ratio'] = df['AveBedrms'] / (df['AveRooms'] + 1e-6)\n",
    "        \n",
    "        # 2. ì¸ë‹¹ ë°© ìˆ˜\n",
    "        df['rooms_per_person'] = df['AveRooms'] / (df['AveOccup'] + 1e-6)\n",
    "        \n",
    "        # 3. ì¸êµ¬ ë°€ë„\n",
    "        df['population_density'] = df['Population'] * df['AveOccup']\n",
    "        \n",
    "        # 4. ì†Œë“ ëŒ€ë¹„ ì£¼íƒ ì—°ë ¹\n",
    "        df['income_age_ratio'] = df['MedInc'] / (df['HouseAge'] + 1)\n",
    "        \n",
    "        # 5. Bay Area ê±°ë¦¬\n",
    "        df['dist_to_bay'] = np.sqrt(\n",
    "            (df['Latitude'] - 37.87)**2 + \n",
    "            (df['Longitude'] + 122.27)**2\n",
    "        )\n",
    "        \n",
    "        # 6. LA ê±°ë¦¬\n",
    "        df['dist_to_la'] = np.sqrt(\n",
    "            (df['Latitude'] - 34.05)**2 + \n",
    "            (df['Longitude'] + 118.24)**2\n",
    "        )\n",
    "        \n",
    "        # 7. í•´ì•ˆ ê·¼ì ‘ë„ (ê²½ë„ ê¸°ì¤€)\n",
    "        df['coastal_proximity'] = np.abs(df['Longitude'] + 122)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    X_train_fe = add_features(X_train)\n",
    "    X_test_fe = add_features(X_test)\n",
    "    \n",
    "    new_cols = [c for c in X_train_fe.columns if c not in original_cols]\n",
    "    print(f\"  New features ({len(new_cols)}): {new_cols}\")\n",
    "    print(f\"  Total features: {len(X_train_fe.columns)}\")\n",
    "    \n",
    "    X_train_fe.to_csv(X_train_out.path, index=False)\n",
    "    X_test_fe.to_csv(X_test_out.path, index=False)\n",
    "    \n",
    "    print(f\"  âœ… Feature engineering completed (7 new features)\")\n",
    "    \n",
    "    return len(new_cols)\n",
    "\n",
    "print(\"âœ… feature_engineering ì •ì˜ ì™„ë£Œ! (7ê°œ í”¼ì²˜)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Component 4: ëª¨ë¸ í•™ìŠµ (AWS ìê²©ì¦ëª… í¬í•¨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.0.3\",\n",
    "        \"scikit-learn==1.3.2\",\n",
    "        \"mlflow==2.9.2\",\n",
    "        \"numpy==1.24.3\",\n",
    "        \"boto3\"\n",
    "    ]\n",
    ")\n",
    "def train_model(\n",
    "    X_train: Input[Dataset],\n",
    "    X_test: Input[Dataset],\n",
    "    y_train: Input[Dataset],\n",
    "    y_test: Input[Dataset],\n",
    "    mlflow_tracking_uri: str,\n",
    "    experiment_name: str,\n",
    "    team_name: str,\n",
    "    # âœ… AWS ìê²©ì¦ëª… íŒŒë¼ë¯¸í„°\n",
    "    aws_access_key_id: str,\n",
    "    aws_secret_access_key: str,\n",
    "    aws_region: str = \"ap-northeast-2\",\n",
    "    n_estimators: int = 100,\n",
    "    max_depth: int = 10\n",
    ") -> str:\n",
    "    \"\"\"ëª¨ë¸ í•™ìŠµ ë° MLflow ê¸°ë¡ (ì™„ì„± ë²„ì „)\"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import mlflow\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "    import os\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Step 4: Train Model - {team_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # âœ… AWS ìê²©ì¦ëª… ì„¤ì •\n",
    "    os.environ['AWS_ACCESS_KEY_ID'] = aws_access_key_id\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = aws_secret_access_key\n",
    "    os.environ['AWS_DEFAULT_REGION'] = aws_region\n",
    "    print(f\"  AWS Credentials: âœ… ì„¤ì •ë¨\")\n",
    "    \n",
    "    X_train_df = pd.read_csv(X_train.path)\n",
    "    X_test_df = pd.read_csv(X_test.path)\n",
    "    y_train_df = pd.read_csv(y_train.path)\n",
    "    y_test_df = pd.read_csv(y_test.path)\n",
    "    \n",
    "    print(f\"  Training data: {X_train_df.shape}\")\n",
    "    print(f\"  Test data: {X_test_df.shape}\")\n",
    "    \n",
    "    os.environ['MLFLOW_TRACKING_URI'] = mlflow_tracking_uri\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"{team_name}-run\") as run:\n",
    "        run_id = run.info.run_id\n",
    "        print(f\"  MLflow Run ID: {run_id}\")\n",
    "        \n",
    "        mlflow.log_params({\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"max_depth\": max_depth,\n",
    "            \"random_state\": 42,\n",
    "            \"n_features\": X_train_df.shape[1],\n",
    "            \"n_samples_train\": X_train_df.shape[0],\n",
    "            \"n_samples_test\": X_test_df.shape[0]\n",
    "        })\n",
    "        mlflow.set_tag(\"team\", team_name)\n",
    "        mlflow.set_tag(\"pipeline\", \"solution\")\n",
    "        \n",
    "        print(f\"  Training RandomForest...\")\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train_df, y_train_df.values.ravel())\n",
    "        \n",
    "        y_pred = model.predict(X_test_df)\n",
    "        \n",
    "        r2 = r2_score(y_test_df, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_df, y_pred))\n",
    "        mae = mean_absolute_error(y_test_df, y_pred)\n",
    "        \n",
    "        mlflow.log_metrics({\"r2\": r2, \"rmse\": rmse, \"mae\": mae})\n",
    "        \n",
    "        print(f\"  Performance:\")\n",
    "        print(f\"    - R2 Score: {r2:.4f}\")\n",
    "        print(f\"    - RMSE: {rmse:.4f}\")\n",
    "        print(f\"    - MAE: {mae:.4f}\")\n",
    "        \n",
    "        # í”¼ì²˜ ì¤‘ìš”ë„\n",
    "        feature_importance = dict(zip(\n",
    "            X_train_df.columns,\n",
    "            model.feature_importances_\n",
    "        ))\n",
    "        sorted_importance = sorted(\n",
    "            feature_importance.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )[:5]\n",
    "        \n",
    "        print(f\"  Top 5 Feature Importance:\")\n",
    "        for feat, imp in sorted_importance:\n",
    "            safe_name = feat.replace(\" \", \"_\")[:15]\n",
    "            mlflow.log_metric(f\"fi_{safe_name}\", imp)\n",
    "            print(f\"    - {feat}: {imp:.4f}\")\n",
    "        \n",
    "        # âœ… ëª¨ë¸ ì €ì¥ (AWS ìê²©ì¦ëª…ìœ¼ë¡œ S3ì— ì €ì¥)\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        print(f\"  âœ… Model saved to MLflow (S3)\")\n",
    "    \n",
    "    return run_id\n",
    "\n",
    "print(\"âœ… train_model ì •ì˜ ì™„ë£Œ! (AWS ìê²©ì¦ëª… í¬í•¨)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Component 5: ëª¨ë¸ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"mlflow==2.9.2\", \"boto3\"]\n",
    ")\n",
    "def evaluate_model(\n",
    "    run_id: str,\n",
    "    mlflow_tracking_uri: str,\n",
    "    r2_threshold: float = 0.75\n",
    ") -> str:\n",
    "    \"\"\"ëª¨ë¸ í‰ê°€ ë° ë°°í¬ ê²°ì •\"\"\"\n",
    "    import mlflow\n",
    "    import os\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Step 5: Evaluate Model\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    os.environ['MLFLOW_TRACKING_URI'] = mlflow_tracking_uri\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "    \n",
    "    run = mlflow.get_run(run_id)\n",
    "    r2 = run.data.metrics.get(\"r2\", 0)\n",
    "    \n",
    "    print(f\"  R2: {r2:.4f}, Threshold: {r2_threshold}\")\n",
    "    \n",
    "    if r2 >= r2_threshold:\n",
    "        print(f\"  âœ… DEPLOY\")\n",
    "        return \"deploy\"\n",
    "    else:\n",
    "        print(f\"  âš ï¸ SKIP\")\n",
    "        return \"skip\"\n",
    "\n",
    "print(\"âœ… evaluate_model ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Component 6: ë°°í¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9-slim\")\n",
    "def deploy_model(run_id: str, model_name: str, namespace: str):\n",
    "    \"\"\"ëª¨ë¸ ë°°í¬ ì •ë³´ ì¶œë ¥\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Step 6: Deploy Model\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Model: {model_name}\")\n",
    "    print(f\"  Namespace: {namespace}\")\n",
    "    print(f\"  Endpoint: http://{model_name}.{namespace}.svc.cluster.local/v1/models/{model_name}:predict\")\n",
    "    print(f\"  âœ… Deployment completed\")\n",
    "\n",
    "print(\"âœ… deploy_model ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Component 7: ì•Œë¦¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9-slim\")\n",
    "def send_alert(run_id: str, team_name: str):\n",
    "    \"\"\"ì„±ëŠ¥ ë¯¸ë‹¬ ì•Œë¦¼\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Alert - {team_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  âš ï¸ Model did not meet performance threshold\")\n",
    "    print(f\"  Recommendations:\")\n",
    "    print(f\"    1. Add more features\")\n",
    "    print(f\"    2. Tune hyperparameters\")\n",
    "    print(f\"    3. Try different algorithms\")\n",
    "\n",
    "print(\"âœ… send_alert ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. íŒŒì´í”„ë¼ì¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"Project Pipeline (Solution)\",\n",
    "    description=\"Solution: E2E ML Pipeline with 7 engineered features + AWS credentials\"\n",
    ")\n",
    "def project_pipeline(\n",
    "    data_source: str = \"sklearn\",\n",
    "    team_name: str = \"solution-team\",\n",
    "    experiment_name: str = \"solution-experiment\",\n",
    "    model_name: str = \"solution-model\",\n",
    "    namespace: str = USER_NAMESPACE,\n",
    "    mlflow_tracking_uri: str = MLFLOW_TRACKING_URI,\n",
    "    # âœ… AWS ìê²©ì¦ëª… íŒŒë¼ë¯¸í„°\n",
    "    aws_access_key_id: str = \"\",\n",
    "    aws_secret_access_key: str = \"\",\n",
    "    aws_region: str = \"ap-northeast-2\",\n",
    "    n_estimators: int = 100,\n",
    "    max_depth: int = 10,\n",
    "    r2_threshold: float = 0.75\n",
    "):\n",
    "    \"\"\"í”„ë¡œì íŠ¸ ì†”ë£¨ì…˜ íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "    \n",
    "    load_task = load_data(data_source=data_source)\n",
    "    \n",
    "    preprocess_task = preprocess(\n",
    "        input_data=load_task.outputs[\"output_data\"]\n",
    "    )\n",
    "    \n",
    "    feature_task = feature_engineering(\n",
    "        X_train_in=preprocess_task.outputs[\"X_train_out\"],\n",
    "        X_test_in=preprocess_task.outputs[\"X_test_out\"]\n",
    "    )\n",
    "    \n",
    "    # âœ… AWS ìê²©ì¦ëª… ì „ë‹¬\n",
    "    train_task = train_model(\n",
    "        X_train=feature_task.outputs[\"X_train_out\"],\n",
    "        X_test=feature_task.outputs[\"X_test_out\"],\n",
    "        y_train=preprocess_task.outputs[\"y_train_out\"],\n",
    "        y_test=preprocess_task.outputs[\"y_test_out\"],\n",
    "        mlflow_tracking_uri=mlflow_tracking_uri,\n",
    "        experiment_name=experiment_name,\n",
    "        team_name=team_name,\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "        aws_region=aws_region,\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth\n",
    "    )\n",
    "    \n",
    "    evaluate_task = evaluate_model(\n",
    "        run_id=train_task.output,\n",
    "        mlflow_tracking_uri=mlflow_tracking_uri,\n",
    "        r2_threshold=r2_threshold\n",
    "    )\n",
    "    \n",
    "    with dsl.If(evaluate_task.output == \"deploy\"):\n",
    "        deploy_model(\n",
    "            run_id=train_task.output,\n",
    "            model_name=model_name,\n",
    "            namespace=namespace\n",
    "        )\n",
    "    \n",
    "    with dsl.If(evaluate_task.output == \"skip\"):\n",
    "        send_alert(\n",
    "            run_id=train_task.output,\n",
    "            team_name=team_name\n",
    "        )\n",
    "\n",
    "print(\"âœ… project_pipeline (Solution) ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. íŒŒì´í”„ë¼ì¸ ì»´íŒŒì¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_file = \"solution_pipeline.yaml\"\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=project_pipeline,\n",
    "    package_path=pipeline_file\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  Solution Pipeline ì»´íŒŒì¼ ì™„ë£Œ!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  íŒŒì¼: {pipeline_file}\")\n",
    "print(\"\")\n",
    "print(\"  âš ï¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œ ì…ë ¥ í•„ìˆ˜:\")\n",
    "print(f\"    - aws_access_key_id\")\n",
    "print(f\"    - aws_secret_access_key\")\n",
    "print(\"\")\n",
    "print(\"  ğŸ“‹ ì†”ë£¨ì…˜ í¬í•¨ ì‚¬í•­:\")\n",
    "print(\"    - 7ê°œ ì—”ì§€ë‹ˆì–´ë§ í”¼ì²˜\")\n",
    "print(\"    - ì™„ì „í•œ MLflow í†µí•©\")\n",
    "print(\"    - í”¼ì²˜ ì¤‘ìš”ë„ ë¡œê¹…\")\n",
    "print(\"    - AWS S3 ëª¨ë¸ ì €ì¥\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. (ì„ íƒ) ì§ì ‘ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_PIPELINE = False\n",
    "\n",
    "if RUN_PIPELINE:\n",
    "    from kfp.client import Client\n",
    "    \n",
    "    client = Client()\n",
    "    \n",
    "    run = client.create_run_from_pipeline_func(\n",
    "        project_pipeline,\n",
    "        arguments={\n",
    "            \"team_name\": TEAM_NAME,\n",
    "            \"experiment_name\": f\"{TEAM_NAME}-experiment\",\n",
    "            \"namespace\": USER_NAMESPACE,\n",
    "            \"mlflow_tracking_uri\": MLFLOW_TRACKING_URI,\n",
    "            \"aws_access_key_id\": AWS_ACCESS_KEY_ID,\n",
    "            \"aws_secret_access_key\": AWS_SECRET_ACCESS_KEY,\n",
    "            \"aws_region\": AWS_REGION,\n",
    "        },\n",
    "        experiment_name=f\"{TEAM_NAME}-experiments\"\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Run ì‹œì‘: {run.run_id}\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  RUN_PIPELINE = Trueë¡œ ë³€ê²½í•˜ì—¬ ì§ì ‘ ì‹¤í–‰í•˜ê±°ë‚˜,\")\n",
    "    print(\"   Kubeflow Dashboardì—ì„œ yaml íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
